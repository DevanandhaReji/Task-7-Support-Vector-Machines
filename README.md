# Task-7-Support-Vector-Machines
Task 7: Support Vector Machines
 Tools & Libraries Used
- Python
- NumPy
- Pandas
- Scikit-learn
- Matplotlib
- Seaborn

---
Step-by-Step Implementation

### ðŸ”¹ **Step 1: Importing Required Libraries**
Imported all essential libraries for data handling, visualization, model training, and evaluation.

### ðŸ”¹ **Step 2: Load and Inspect the Dataset**
- Loaded the CSV using `pandas`
- Displayed the top rows and checked for null values
- Dropped unnecessary column (`id`)
- Converted target labels to numeric (`diagnosis`: M â†’ 1, B â†’ 0)

### ðŸ”¹ **Step 3: Feature Scaling**
- Applied `StandardScaler` to normalize features
- Scaling is important for SVM as itâ€™s sensitive to feature magnitudes

### ðŸ”¹ **Step 4: Splitting the Dataset**
- Used `train_test_split` to split the data:
  - 80% for training
  - 20% for testing

### ðŸ”¹ **Step 5: Model 1 - SVM with Linear Kernel**
- Trained an SVM with `kernel='linear'`
- Made predictions and evaluated with:
  - Accuracy Score
  - Classification Report

### ðŸ”¹ **Step 6: Model 2 - SVM with RBF Kernel**
- Trained an SVM with `kernel='rbf'` for non-linear classification
- Evaluated performance as above

### ðŸ”¹ **Step 7: Hyperparameter Tuning**
- Used `GridSearchCV` to tune `C` and `gamma` for RBF SVM
- Performed 5-fold cross-validation
- Identified the best parameters and evaluated test accuracy

### ðŸ”¹ **Step 8: Cross-Validation**
- Performed cross-validation on the tuned model
- Reported individual fold scores and average accuracy

### ðŸ”¹ **Step 9: Confusion Matrix**
- Visualized true positives, false positives, true negatives, and false negatives
- Helped in understanding misclassification

